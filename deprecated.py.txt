UPLOAD_LORA_INIT  = f"{API_BASE}/index.php?r=lora/uploadLoraInit"
UPLOAD_LORA_CHUNK  = f"{API_BASE}/index.php?r=lora/uploadLoraChunk"
UPLOAD_LORA_FINAL  = f"{API_BASE}/index.php?r=lora/uploadLoraFinal"


            elif ttype == "lora_train":
                # üî• –Ω–æ–≤–∏–π —Ç–∏–ø –∑–∞–¥–∞—á—ñ
                handle_lora_train_task(task)

def handle_lora_train_task(task):
    tid = task["id"]
    workflow_key = task["workflow_key"]
    payload = task.get("payload") or {}

    lora_name = payload.get("lora_name")
    if not lora_name:
        raise RuntimeError("payload.lora_name –æ–±–æ–≤ º—è–∑–∫–æ–≤–∏–π")

    # === –¥–µ comfy –∑–±–µ—Ä—ñ–≥–∞—î –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ ===
    # –†–µ–∫–æ–º–µ–Ω–¥—É—é –ù–ï /opt/output, –∞ volume, –∞–ª–µ –ª–∏—à–∞—é —è–∫ —Ç–∏ –Ω–∞–ø–∏—Å–∞–≤
    out_model_path = f"/opt/output/{tid}.safetensors_rank16_fp16.safetensors"

    # === –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ upload ===
    character_folder = payload.get("character_name") or payload.get("character_id") or lora_name

    # 1) –Ø–∫—â–æ —Ñ–∞–π–ª –≤–∂–µ —î ‚Äî –ù–ï —Ç—Ä–µ–Ω—É—î–º–æ, –æ–¥—Ä–∞–∑—É upload
    if os.path.exists(out_model_path) and os.path.getsize(out_model_path) > 10_000_000:
        log(f"[LoRA #{tid}] –§–∞–π–ª –≤–∂–µ —ñ—Å–Ω—É—î: {out_model_path} ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è, —Ä–æ–±–ª—é upload.")
        update_task(tid, "running", payload_update={"stage": "upload_existing_model"})
        upload_samples(tid)
        up = upload_chunked(
            file_path=out_model_path,
            lora_name=lora_name,
        )
        update_task(tid, "done", None, {
            "note": "LoRA uploaded (training skipped, file existed)",
            "lora_model_local": out_model_path,
        })
        log(f"‚úÖ LoRA-–∑–∞–¥–∞—á–∞ #{tid} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (skip train), upload: {up.get('path')}")
        return

    clear_dir()

    # 3) –ó–∞–ø—É—Å–∫–∞—î–º–æ comfy training
    log(f"[LoRA #{tid}] –°—Ç–∞—Ä—Ç —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —á–µ—Ä–µ–∑ Comfy, workflow={workflow_key}")
    update_task(tid, "running", payload_update={"stage": "comfy_training_started"})

    result = run_comfy_training_workflow(workflow_key, payload, timeout_sec=7200)

    log(f"[LoRA #{tid}] –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–µ —à—É–∫–∞—é —Ñ–∞–π–ª {out_model_path}")
    # 4) –ß–µ–∫–∞—î–º–æ —â–æ–± —Ñ–∞–π–ª —Ä–µ–∞–ª—å–Ω–æ –∑ º—è–≤–∏–≤—Å—è
    if not wait_for_file(out_model_path, timeout_sec=600, min_size=1_000_000):
        raise RuntimeError(f"[LoRA #{tid}] Comfy –∑–∞–≤–µ—Ä—à–∏–≤—Å—è, –∞–ª–µ —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ/–∑–∞–º–∞–ª–∏–π: {out_model_path}")

    log(f"[LoRA #{tid}] –§–∞–π–ª –∑–Ω–∞–π–¥–µ–Ω–æ")
    # 5) Upload
    update_task(tid, "running", payload_update={"stage": "upload_trained_model", "comfy_id": result.get("id")})
    
    upload_samples(tid)
    log(f"[LoRA #{tid}] Samples –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
    up = upload_chunked(
        file_path=out_model_path,
        lora_name=lora_name,
    )
    log(f"[LoRA #{tid}] –§–∞–π–ª –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")

    payload_update = {
        "note": "LoRA training done via comfyui-api and uploaded",
        "comfy_id": result.get("id"),
        "stats": result.get("stats"),
        "lora_model_local": out_model_path,
        "lora_model_remote": up.get("path"),
        "remote_size": up.get("size"),
    }

    update_task(tid, "done", None, payload_update)
    log(f"‚úÖ LoRA-–∑–∞–¥–∞—á–∞ #{tid} –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –º–æ–¥–µ–ª—å: {out_model_path} ‚Üí {up.get('path')}")