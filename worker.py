#pip install requests
#python3 worker.py


import os
import time
import json
import uuid
import traceback
import requests
import base64
import subprocess
import hashlib
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# ------------------ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è ------------------

API_BASE = os.environ["API_BASE"]
API_TOKEN = os.environ["API_TOKEN"]

GET_TASK_URL      = f"{API_BASE}/index.php?r=worker/getTask"
UPDATE_TASK_URL   = f"{API_BASE}/index.php?r=worker/updateTask"
UPLOAD_IMAGE_URL  = f"{API_BASE}/index.php?r=worker/uploadImage"
UPLOAD_FILE_URL   = f"{API_BASE}/index.php?r=worker/uploadFile"
UPLOAD_LORA_INIT  = f"{API_BASE}/index.php?r=lora/uploadLoraInit"
UPLOAD_LORA_CHUNK  = f"{API_BASE}/index.php?r=lora/uploadLoraChunk"
UPLOAD_LORA_FINAL  = f"{API_BASE}/index.php?r=lora/uploadLoraFinal"

COMFY_SERVER = "127.0.0.1:3000"            # ComfyUI –Ω–∞ Salad-—Å–µ—Ä–≤–µ—Ä—ñ
COMFY_HTTP   = f"http://{COMFY_SERVER}"

CHECK_INTERVAL = 5                         # —Å–µ–∫. –ø–∞—É–∑–∞ –º—ñ–∂ —Ü–∏–∫–ª–∞–º–∏

TMP_DIR = "/tmp/comfy_worker"
WORKFLOWS_DIR = "/opt/comfy_workflows"
os.makedirs(TMP_DIR, exist_ok=True)

TRAIN_DATA_DIR = "/opt/lora_train_data"
TRAIN_OUTPUT_DIR = "/opt/lora_train_output"
os.makedirs(TRAIN_DATA_DIR, exist_ok=True)
os.makedirs(TRAIN_OUTPUT_DIR, exist_ok=True)
DOWNLOAD_FILE_URL = f"{API_BASE}/index.php?r=worker/getFile"

# ------------------ –°–µ—Ä–≤—ñ—Å–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó ------------------

def sha256_file(path, chunk=1024*1024):
    h = hashlib.sha256()
    with open(path, "rb") as f:
        while True:
            b = f.read(chunk)
            if not b:
                break
            h.update(b)
    return h.hexdigest()

def log(msg: str):
    ts = datetime.now().strftime("%H:%M:%S")
    print(f"[{ts}] {msg}", flush=True)


def get_task():
    try:
        r = requests.post(GET_TASK_URL, data={"token": API_TOKEN}, timeout=15)
        r.raise_for_status()
        data = r.json()
        if not data.get("success"):
            return None
        return data.get("task")
    except Exception as e:
        log(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Ç—É –∑–∞–¥–∞—á—ñ: {e}")
        return None


def update_task(task_id, status, error=None, payload_update=None):
    payload = {
        "token": API_TOKEN,
        "id": task_id,
        "status": status,
    }
    if error:
        payload["error_message"] = error
    if payload_update is not None:
        payload["payload_update"] = json.dumps(payload_update, ensure_ascii=False)
    try:
        requests.post(UPDATE_TASK_URL, data=payload, timeout=15)
    except Exception as e:
        log(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–Ω–æ–≤–∏—Ç–∏ —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á—ñ {task_id}: {e}")

def upload_file(task_id, path):
    files = {"file": open(path, "rb")}
    data = {"token": API_TOKEN, "task_id": task_id}
    try:
        r = requests.post(UPLOAD_FILE_URL, data=data, files=files, timeout=120)
        r.raise_for_status()
        resp = r.json()
        return None
    except Exception as e:
        log(f"–ü–æ–º–∏–ª–∫–∞ –∞–ø–ª–æ–∞–¥—É —Ñ–∞–π–ª—É {path}: {e}")
        return None
    finally:
        files["file"].close()

def upload_image(task_id, path):
    files = {"file": open(path, "rb")}
    data = {"token": API_TOKEN, "task_id": task_id}
    try:
        r = requests.post(UPLOAD_IMAGE_URL, data=data, files=files, timeout=120)
        r.raise_for_status()
        resp = r.json()
        return resp.get("result_path")
    except Exception as e:
        log(f"–ü–æ–º–∏–ª–∫–∞ –∞–ø–ª–æ–∞–¥—É —Ñ–∞–π–ª—É {path}: {e}")
        return None
    finally:
        files["file"].close()

def upload_samples(task_id, samples_dir="/opt/output/sample"):
    if not os.path.isdir(samples_dir):
        print(f"[INFO] samples dir not found: {samples_dir}")
        return

    for filename in os.listdir(samples_dir):
        file_path = os.path.join(samples_dir, filename)

        if not os.path.isfile(file_path):
            continue

        try:
            upload_file(task_id, file_path)
            print(f"[OK] uploaded: {file_path}")
        except Exception as e:
            print(f"[ERROR] failed to upload {file_path}: {e}")

def upload_lora_chunked(
    file_path: str,
    lora_name: str,
    chunk_size: int = 2 * 1024 * 1024,
    max_retries: int = 8,
):
    total_size = os.path.getsize(file_path)
    file_hash = sha256_file(file_path)

    headers = {"X-Auth-Token": API_TOKEN}

    # 1) init / resume
    r = requests.post(UPLOAD_LORA_INIT, headers=headers, data={
        "lora_name": lora_name,
        "total_size": str(total_size),
        "sha256": file_hash,
    }, timeout=30)
    r.raise_for_status()
    j = r.json()
    if j.get("status") != "ok":
        raise RuntimeError(j)

    uploaded = int(j["uploaded_bytes"])
    print(f"[upload] resume from {uploaded}/{total_size}")

    # 2) upload chunks append-only
    with open(file_path, "rb") as f:
        f.seek(uploaded)
        offset = uploaded

        while offset < total_size:
            data = f.read(chunk_size)
            if not data:
                break

            # retry loop
            attempt = 0
            while True:
                try:
                    rr = requests.post(
                        UPLOAD_LORA_CHUNK,
                        headers={**headers, "Content-Type": "application/octet-stream"},
                        params={"lora_name": lora_name, "offset": str(offset)},
                        data=data,
                        timeout=120,
                    )
                    # 409 = offset mismatch -> –ø–æ–≤—Ç–æ—Ä–Ω–æ init —ñ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –∑ correct offset
                    if rr.status_code == 409:
                        jj = rr.json()
                        offset = int(jj.get("expected_offset", offset))
                        f.seek(offset)
                        print(f"[upload] offset mismatch, jump to {offset}")
                        data = f.read(chunk_size)
                        continue

                    rr.raise_for_status()
                    jj = rr.json()
                    if jj.get("status") != "ok":
                        raise RuntimeError(jj)

                    offset = int(jj["uploaded_bytes"])
                    print(f"[upload] {offset}/{total_size}")
                    break
                except Exception as e:
                    attempt += 1
                    if attempt > max_retries:
                        raise
                    sleep = min(2 ** attempt, 30)
                    print(f"[upload] retry {attempt}/{max_retries} after {sleep}s: {e}")
                    time.sleep(sleep)

    # 3) finalize
    rf = requests.post(UPLOAD_LORA_FINAL, headers=headers, data={
        "lora_name": lora_name,
        "total_size": str(total_size),
        "sha256": file_hash,
    }, timeout=60)
    rf.raise_for_status()
    jf = rf.json()
    if jf.get("status") != "ok":
        raise RuntimeError(jf)

    print(f"[upload] DONE: {jf.get('path')} size={jf.get('size')}")
    return jf

# ------------------ Lora train
def download_training_file(name: str) -> str:
    params = {
        "token": API_TOKEN,
        "name": name,
    }
    try:
        r = requests.post(DOWNLOAD_FILE_URL, params=params, timeout=600, stream=True)
        r.raise_for_status()
    except Exception as e:
        raise RuntimeError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ñ–∞–π–ª {name}: {e}")

    local_path = os.path.join(TRAIN_DATA_DIR, name)
    # –Ω–∞ –≤—Å—è–∫ –≤–∏–ø–∞–¥–æ–∫ ‚Äî —Å—Ç–≤–æ—Ä–∏–º–æ –ø—ñ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó, —è–∫—â–æ –≤ —ñ–º–µ–Ω—ñ —î —à–ª—è—Ö
    os.makedirs(os.path.dirname(local_path), exist_ok=True)

    with open(local_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)

    log(f"–§–∞–π–ª {name} –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {local_path}")
    return local_path

def _download_one(session: requests.Session, name: str) -> str:
    params = {"token": API_TOKEN, "name": name}

    local_path = os.path.join(TRAIN_DATA_DIR, name)
    os.makedirs(os.path.dirname(local_path), exist_ok=True)

    with session.post(DOWNLOAD_FILE_URL, params=params, timeout=600, stream=True) as r:
        r.raise_for_status()
        with open(local_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=1024 * 1024):  # 1MB
                if chunk:
                    f.write(chunk)

    return local_path


def download_training_files(file_prefix, names, max_workers=16, retries=5):
    """
    –ü–∞—Ä–∞–ª–µ–ª—å–Ω–µ —Å–∫–∞—á—É–≤–∞–Ω–Ω—è –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º max_workers.
    –ü–æ–≤–µ—Ä—Ç–∞—î (ok_paths, failed_names).
    """
    to_download = [file_prefix + n for n in names]
    ok_paths = []
    failed = []

    # –í–ê–ñ–õ–ò–í–û: Session –Ω–µ thread-safe, —Ç–æ–º—É —Ä–æ–±–∏–º–æ –ø–æ —Å–µ—Å—ñ—ó –Ω–∞ –ø–æ—Ç—ñ–∫ —á–µ—Ä–µ–∑ initializer-–ª–∞–π—Ç.
    # –ù–∞–π–ø—Ä–æ—Å—Ç—ñ—à–µ ‚Äî —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ session –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∑–∞–¥–∞—á—ñ (—Ç—Ä–æ—Ö–∏ –¥–æ—Ä–æ–∂—á–µ), –∞–±–æ —Ç—Ä–∏–º–∞—Ç–∏ thread-local.
    import threading
    tls = threading.local()

    def task(full_name: str) -> str:
        if not hasattr(tls, "session"):
            tls.session = requests.Session()

        last_err = None
        for attempt in range(retries):
            try:
                return _download_one(tls.session, full_name)
            except requests.HTTPError as e:
                status = getattr(e.response, "status_code", None)
                # retry –Ω–∞ rate limit / —Ç–∏–º—á–∞—Å–æ–≤—ñ —Å–µ—Ä–≤–µ—Ä–Ω—ñ
                if status in (429, 500, 502, 503, 504):
                    sleep_s = min(60, (2 ** attempt) + random.random())
                    time.sleep(sleep_s)
                    last_err = e
                    continue
                raise
            except (requests.ConnectionError, requests.Timeout) as e:
                sleep_s = min(60, (2 ** attempt) + random.random())
                time.sleep(sleep_s)
                last_err = e
                continue

        raise RuntimeError(f"Failed after {retries} retries: {full_name}. Last error: {last_err}")

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(task, n): n for n in to_download}
        for fut in as_completed(futures):
            n = futures[fut]
            try:
                p = fut.result()
                ok_paths.append(p)
                # —è–∫—â–æ —Ö–æ—á–µ—à –ø—Ä–æ–≥—Ä–µ—Å:
                # log(f"OK: {n}")
            except Exception as e:
                failed.append(n)
                log(f"FAIL: {n}: {e}")

    return ok_paths, failed


# ------------------ ComfyUI —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è ------------------
# API: /prompt, /history/{id}, /view?filename=...&subfolder=...&type=... :contentReference[oaicite:0]{index=0}


def build_workflow_from_payload(workflow_key: str, payload: dict) -> dict:
    path = os.path.join(WORKFLOWS_DIR, f"{workflow_key}.json")
    if not os.path.isfile(path):
        raise FileNotFoundError(f"Workflow template not found: {path}")

    with open(path, "r", encoding="utf-8") as f:
        txt = f.read()

    # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –í–°–Ü–• –∫–ª—é—á–∞—Ö payload
    for key, value in payload.items():
        placeholder = f"param_{key}"

        # None ‚Üí null
        if value is None:
            replacement = "null"
        # —á–∏—Å–ª–∞
        elif isinstance(value, (int, float)):
            replacement = str(value)
        else:
            s = str(value)
            dumped = json.dumps(s, ensure_ascii=False)
            replacement = dumped[1:-1]  # –≤–∏–∫–∏–¥–∞—î–º–æ –∑–æ–≤–Ω—ñ—à–Ω—ñ –ª–∞–ø–∫–∏

        # –Ü —Ç—É–ø–æ –∑–∞–º—ñ–Ω—è—î–º–æ –≤ —Ç–µ–∫—Å—Ç—ñ –í–°–Ü –≤—Ö–æ–¥–∂–µ–Ω–Ω—è param_<key>
        txt = txt.replace(placeholder, replacement)

    # –¢–µ–ø–µ—Ä —Ü–µ –≤–∂–µ –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –≤–∞–ª—ñ–¥–Ω–∏–π JSON
    try:
        workflow = json.loads(txt)
    except json.JSONDecodeError as e:
        raise ValueError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ workflow –ø—ñ—Å–ª—è –ø—ñ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏: {e}\n–®–º–∞—Ç–æ–∫: {txt[:300]}")

    return workflow

def queue_prompt_to_comfy(workflow: dict, client_id: str) -> str:
    """
    –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ workflow –≤ ComfyUI —á–µ—Ä–µ–∑ /prompt.
    –ü–æ–≤–µ—Ä—Ç–∞—î prompt_id.
    """
    url = f"{COMFY_HTTP}/prompt"
    payload = {
        "prompt": workflow,
        "client_id": client_id,
    }
    r = requests.post(url, json=payload, timeout=600)
    r.raise_for_status()
    data = r.json()
    prompt_id = data.get("prompt_id")
    if not prompt_id:
        raise RuntimeError(f"ComfyUI –Ω–µ –ø–æ–≤–µ—Ä–Ω—É–≤ prompt_id: {data}")
    return prompt_id

def run_comfy_training_workflow(workflow_key: str, payload: dict, timeout_sec: int = 7200) -> dict:
    client_id = str(uuid.uuid4())

    workflow = build_workflow_from_payload(workflow_key, payload)
    url = f"{COMFY_HTTP}/prompt"

    body = {
        "prompt": workflow,
        # –ú–æ–∂–µ—à –∑–∞–¥–∞—Ç–∏ —Å–≤—ñ–π id –¥–ª—è —Ç—Ä–µ–π—Å—ñ–Ω–≥—É:
        "id": str(uuid.uuid4()),
    }

    # timeout = (connect_timeout, read_timeout)
    r = requests.post(url, json=body, timeout=(5, timeout_sec))

    if r.status_code >= 400:
        raise RuntimeError(f"comfyui-api –ø–æ–º–∏–ª–∫–∞ {r.status_code}: {r.text}")

    data = r.json()
    # –î–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–æ–±—ñ –Ω–µ –æ–±–æ–≤'—è–∑–∫–æ–≤–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ images,
    # ComfyUI workflow –º–æ–∂–µ –ø—Ä–æ—Å—Ç–æ –∑–±–µ—Ä–µ–≥—Ç–∏ LoRA –Ω–∞ –¥–∏—Å–∫.
    return data



def run_workflow_via_comfy_api(workflow: dict, client_id: str) -> dict:
    url = f"{COMFY_HTTP}/prompt"
    payload = {
        "prompt": workflow,
        "client_id": client_id,
    }
    r = requests.post(url, json=payload, timeout=(5, 600))

    if r.status_code >= 400:
        # —Ç—É—Ç –ø–æ–±–∞—á–∏–º–æ —Å–ø—Ä–∞–≤–∂–Ω—é –ø—Ä–∏—á–∏–Ω—É 500
        raise RuntimeError(
            f"comfyui-api –ø–æ–º–∏–ª–∫–∞ {r.status_code}: {r.text}"
        )

    data = r.json()
    if "images" not in data:
        raise RuntimeError(f"–ù–µ—Å–ø–æ–¥—ñ–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ comfyui-api: {data}")
    return data


def wait_for_result(prompt_id: str, timeout_sec: int = 600) -> dict:
    """
    –ü–æ–ª—ñ–Ω–≥ /history/{prompt_id}, –ø–æ–∫–∏ –Ω–µ –±—É–¥–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É.
    –ü–æ–≤–µ—Ä—Ç–∞—î JSON history.
    """
    url = f"{COMFY_HTTP}/history/{prompt_id}"
    start = time.time()
    while True:
        r = requests.get(url, timeout=15)
        if r.status_code == 200:
            data = r.json()
            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞: { prompt_id: { "outputs": {...} } }
            if prompt_id in data and "outputs" in data[prompt_id]:
                return data[prompt_id]
        # —á–µ–∫
        if time.time() - start > timeout_sec:
            raise TimeoutError(f"ComfyUI –Ω–µ –∑–∞–≤–µ—Ä—à–∏–≤ –∑–∞–¥–∞—á—É –∑–∞ {timeout_sec} —Å–µ–∫.")
        time.sleep(2)


def extract_first_image_info(history: dict) -> dict:
    """
    –í–∏—Ç—è–≥—É—î–º–æ –ø–µ—Ä—à–∏–π output image: filename, subfolder, type.
    """
    outputs = history.get("outputs", {})
    for node_id, node_out in outputs.items():
        images = node_out.get("images") or []
        if not images:
            continue
        img = images[0]
        return {
            "filename": img.get("filename"),
            "subfolder": img.get("subfolder") or "",
            "type": img.get("type") or "output",
        }
    raise RuntimeError("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ history")


def download_image_from_comfy(info: dict, local_path: str):
    """
    /view?filename=...&subfolder=...&type=...
    """
    params = {
        "filename": info["filename"],
        "subfolder": info["subfolder"],
        "type": info["type"],  # input/output/temp
    }
    url = f"{COMFY_HTTP}/view"
    r = requests.get(url, params=params, timeout=120)
    r.raise_for_status()
    with open(local_path, "wb") as f:
        f.write(r.content)


def generate_with_comfy(workflow_key: str, payload: dict) -> str:
    """
    –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª:
      1) –ø–æ–±—É–¥—É–≤–∞—Ç–∏ workflow_json
      2) /prompt -> prompt_id
      3) —á–µ–∫–∞—Ç–∏ /history/prompt_id
      4) –∑–∞–±—Ä–∞—Ç–∏ –ø–µ—Ä—à–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —á–µ—Ä–µ–∑ /view
      5) –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–∏–π —à–ª—è—Ö –¥–æ PNG
    """
    client_id = str(uuid.uuid4())

    # 1) –±—É–¥—É—î–º–æ workflow –∑ payload
    workflow = build_workflow_from_payload(workflow_key, payload)

    # 2) –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –≤ ComfyUI
    # prompt_id = queue_prompt_to_comfy(workflow, client_id)
    # log(f"ComfyUI prompt_id={prompt_id}")

    # # 3) —á–µ–∫–∞—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
    # history = wait_for_result(prompt_id)

    # # 4) –±–µ—Ä–µ–º–æ –ø–µ—Ä—à–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    # img_info = extract_first_image_info(history)
    # log(f"–û—Ç—Ä–∏–º–∞–Ω–æ —Ñ–∞–π–ª –∑ ComfyUI: {img_info}")

    # # 5) –∫–∞—á–∞—î–º–æ –≤ tmp
    # ext = os.path.splitext(img_info["filename"])[1] or ".png"
    # tmp_name = f"comfy_{prompt_id[:8]}{ext}"
    # local_path = os.path.join(TMP_DIR, tmp_name)
    # download_image_from_comfy(img_info, local_path)

    # 2) –∑–∞–ø—É—Å–∫–∞—î–º–æ workflow —á–µ—Ä–µ–∑ comfyui-api
    result = run_workflow_via_comfy_api(workflow, client_id)
    task_id = result.get("id")
    log(f"comfyui-api task_id={task_id}")

    images = result.get("images") or []
    if not images:
        raise RuntimeError(f"comfyui-api –Ω–µ –ø–æ–≤–µ—Ä–Ω—É–≤ images: {result}")

    # 3) –±–µ—Ä–µ–º–æ –ø–µ—Ä—à–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    first = images[0]

    # comfyui-api –º–æ–∂–µ –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ –∞–±–æ —á–∏—Å—Ç–∏–π base64-—Ä—è–¥–æ–∫,
    # –∞–±–æ dict –∑ –ø–æ–ª—è–º–∏ —Ç–∏–ø—É {"image": "...", "filename": "..."}
    if isinstance(first, dict):
        b64_data = first.get("image") or first.get("data")
        filename = first.get("filename") or f"{task_id}.png"
    else:
        b64_data = first
        filename = f"{task_id}.png"

    if not b64_data:
        raise RuntimeError(f"–ù–µ–º–∞—î base64 –¥–∞–Ω–∏—Ö —É images[0]: {first}")

    # 4) –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ TMP_DIR
    ext = os.path.splitext(filename)[1] or ".png"
    safe_id = (task_id or "comfy")[:8]
    tmp_name = f"comfy_{safe_id}{ext}"
    local_path = os.path.join(TMP_DIR, tmp_name)

    os.makedirs(TMP_DIR, exist_ok=True)
    with open(local_path, "wb") as f:
        f.write(base64.b64decode(b64_data))

    log(f"–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ: {local_path}")
    return local_path


# ------------------ –ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª ------------------
def wait_for_file(path: str, timeout_sec: int = 300, min_size: int = 10_000_000):
    """–ß–µ–∫–∞—î –ø–æ—è–≤–∏ —Ñ–∞–π–ª—É —ñ —â–æ–± –≤—ñ–Ω –±—É–≤ –Ω–µ –ø—É—Å—Ç–∏–π/–Ω–µ –±–∏—Ç–∏–π (min_size)."""
    t0 = time.time()
    while time.time() - t0 < timeout_sec:
        if os.path.exists(path):
            try:
                if os.path.getsize(path) >= min_size:
                    return True
            except OSError:
                pass
        time.sleep(2)
    return False

def handle_lora_train_task(task):
    tid = task["id"]
    workflow_key = task["workflow_key"]
    payload = task.get("payload") or {}

    lora_name = payload.get("lora_name")
    if not lora_name:
        raise RuntimeError("payload.lora_name –æ–±–æ–≤ º—è–∑–∫–æ–≤–∏–π")

    # === –¥–µ comfy –∑–±–µ—Ä—ñ–≥–∞—î –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ ===
    # –†–µ–∫–æ–º–µ–Ω–¥—É—é –ù–ï /opt/output, –∞ volume, –∞–ª–µ –ª–∏—à–∞—é —è–∫ —Ç–∏ –Ω–∞–ø–∏—Å–∞–≤
    out_model_path = f"/opt/output/{lora_name}.safetensors_rank16_fp16.safetensors"

    # === –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ upload ===
    character_folder = payload.get("character_name") or payload.get("character_id") or lora_name

    # 1) –Ø–∫—â–æ —Ñ–∞–π–ª –≤–∂–µ —î ‚Äî –ù–ï —Ç—Ä–µ–Ω—É—î–º–æ, –æ–¥—Ä–∞–∑—É upload
    if os.path.exists(out_model_path) and os.path.getsize(out_model_path) > 10_000_000:
        log(f"[LoRA #{tid}] –§–∞–π–ª –≤–∂–µ —ñ—Å–Ω—É—î: {out_model_path} ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è, —Ä–æ–±–ª—é upload.")
        update_task(tid, "running", payload_update={"stage": "upload_existing_model"})
        upload_samples(tid)
        upload_lora_chunked(
            file_path=out_model_path,
            lora_name=lora_name,
        )
        update_task(tid, "done", None, {
            "note": "LoRA uploaded (training skipped, file existed)",
            "lora_model_local": out_model_path,
        })
        log(f"‚úÖ LoRA-–∑–∞–¥–∞—á–∞ #{tid} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (skip train), upload: {up.get('path')}")
        return

    # 2) –°—Ç–∞—Ä–∏–π —à–ª—è—Ö –∑ zip/—Ñ–∞–π–ª–∞–º–∏ ‚Äî –ª–∏—à–∞—é —è–∫ optional fallback
    file_names = payload.get("files") or []
    file_prefix = payload.get("files_prefix")
    if file_names:
        log(f"[LoRA #{tid}] (fallback) –°–∫–∞—á—É—î–º–æ —Ñ–∞–π–ª–∏: {file_names}")
        #data_paths = 
        download_training_files(file_prefix, file_names)
        #payload["data_paths"] = data_paths  # —è–∫—â–æ comfy/workflow —Ü–µ —á–∏—Ç–∞—î
    else:
        log(f"[LoRA #{tid}] payload.files –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Äî –ø—Ä–∏–ø—É—Å–∫–∞—é, —â–æ dataset –≤–∂–µ –Ω–∞ –¥–∏—Å–∫—É/–Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π —É workflow.")

    # 3) –ó–∞–ø—É—Å–∫–∞—î–º–æ comfy training
    log(f"[LoRA #{tid}] –°—Ç–∞—Ä—Ç —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —á–µ—Ä–µ–∑ Comfy, workflow={workflow_key}")
    update_task(tid, "running", payload_update={"stage": "comfy_training_started"})

    result = run_comfy_training_workflow(workflow_key, payload, timeout_sec=7200)

    log(f"[LoRA #{tid}] –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–µ —à—É–∫–∞—é —Ñ–∞–π–ª {out_model_path}")
    # 4) –ß–µ–∫–∞—î–º–æ —â–æ–± —Ñ–∞–π–ª —Ä–µ–∞–ª—å–Ω–æ –∑ º—è–≤–∏–≤—Å—è
    if not wait_for_file(out_model_path, timeout_sec=600, min_size=1_000_000):
        raise RuntimeError(f"[LoRA #{tid}] Comfy –∑–∞–≤–µ—Ä—à–∏–≤—Å—è, –∞–ª–µ —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ/–∑–∞–º–∞–ª–∏–π: {out_model_path}")

    log(f"[LoRA #{tid}] –§–∞–π–ª –∑–Ω–∞–π–¥–µ–Ω–æ")
    # 5) Upload
    update_task(tid, "running", payload_update={"stage": "upload_trained_model", "comfy_id": result.get("id")})
    
    upload_samples(tid)
    log(f"[LoRA #{tid}] Samples –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
    up = upload_lora_chunked(
        file_path=out_model_path,
        lora_name=lora_name,
    )
    log(f"[LoRA #{tid}] –§–∞–π–ª –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")

    payload_update = {
        "note": "LoRA training done via comfyui-api and uploaded",
        "comfy_id": result.get("id"),
        "stats": result.get("stats"),
        "lora_model_local": out_model_path,
        "lora_model_remote": up.get("path"),
        "remote_size": up.get("size"),
    }

    update_task(tid, "done", None, payload_update)
    log(f"‚úÖ LoRA-–∑–∞–¥–∞—á–∞ #{tid} –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –º–æ–¥–µ–ª—å: {out_model_path} ‚Üí {up.get('path')}")

def main():
    log("–í–æ—Ä–∫–µ—Ä –∑–∞–ø—É—â–µ–Ω–æ. –û—á—ñ–∫—É—î–º–æ –∑–∞–¥–∞—á—ñ...")
    while True:
        task = get_task()
        if not task:
            time.sleep(CHECK_INTERVAL)
            continue

        tid = task["id"]
        ttype = task["type"]
        workflow_key = task["workflow_key"]
        payload = task["payload"] or {}

        try:
            log(f"–û—Ç—Ä–∏–º–∞–Ω–æ –∑–∞–¥–∞—á—É #{tid} [{ttype}] workflow={workflow_key}")

            # –ø—Ä–∏–∫–ª–∞–¥: type == 'lora_image' –∞–±–æ 'frame_image' ‚Äî –≤—Å–µ –æ–¥–Ω–æ, –º–∏ –ø—Ä–æ—Å—Ç–æ —à–ª–µ–º–æ –≤ Comfy
            if ttype in ("lora_image", "frame_image", "other"):
                local_path = generate_with_comfy(workflow_key, payload)
                remote_path = upload_image(tid, local_path)
                if remote_path:
                    update_task(tid, "done", None, {"result_path": remote_path})
                    log(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞–¥–∞—á—É #{tid}, result={remote_path}")
                else:
                    update_task(tid, "failed", "Upload failed")
            elif ttype == "lora_train":
                # üî• –Ω–æ–≤–∏–π —Ç–∏–ø –∑–∞–¥–∞—á—ñ
                handle_lora_train_task(task)
            else:
                update_task(tid, "failed", f"–ù–µ–≤—ñ–¥–æ–º–∏–π —Ç–∏–ø –∑–∞–¥–∞—á—ñ: {ttype}")

        except NotImplementedError as e:
            # —Ç–∏ —â–µ –Ω–µ —Ä–µ–∞–ª—ñ–∑—É–≤–∞–≤ build_workflow_from_payload
            log(f"‚ùå build_workflow_from_payload –Ω–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π: {e}")
            update_task(tid, "failed", "Workflow builder not implemented")
        except Exception as e:
            err = traceback.format_exc()
            log(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–¥–∞—á—ñ #{tid}: {e}")
            update_task(tid, "failed", err)

        time.sleep(1)


if __name__ == "__main__":
    main()
